{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Technology\\\\AI\\\\Python\\\\Gen AI\\\\LangChain\\\\GenAI-SourceCode-Analysis\\\\research'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/amirkh69/GenAI-Medical-ChatBot.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\ test.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\n#from langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\nfrom langchain.vectorstores import Pinecone\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Amir Khan',\\n    author_email= 'eamirkh360@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\n#from langchain_pinecone import Pinecone\\n#from langchain.vectorstores import Pinecone\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\n#PINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n#os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nextracted_data=load_pdf_file(data=\\'Data/\\')\\ntext_chunks=text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\nindex_name = \"medicalbot\"\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\n#pc = Pinecone(pinecone_api_key=PINECONE_API_KEY,embedding=embeddings,index_name=index_name)\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, \\n    metric=\"cosine\", \\n    spec=ServerlessSpec(\\n        cloud=\"aws\", \\n        region=\"us-east-1\"\\n    ) \\n) \\n\\nfrom langchain.vectorstores import Pinecone\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = Pinecone.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings, \\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='#from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n#from langchain.embeddings import HuggingFaceEmbeddings\\n\\nfrom langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\n\\n#Extract Data From the PDF File\\ndef load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\n#Download the Embeddings from HuggingFace \\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')  #this model return 384 dimensions\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\n\\nsystem_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\n#from langchain_pinecone import PineconeVectorStore\\nfrom langchain_openai import OpenAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain_core.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\nload_dotenv()'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='load_dotenv()\\n\\nPINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\nos.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\\n\\nembeddings = download_hugging_face_embeddings()\\n\\n\\nindex_name = \"medicalbot\"\\n\\nfrom langchain.vectorstores import Pinecone'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = Pinecone.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\\n\\n\\nllm = OpenAI(temperature=0.4, max_tokens=500)\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='question_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"GET\", \"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    input = msg\\n    print(input)\\n    response = rag_chain.invoke({\"input\": msg})\\n    print(\"Response : \", response[\"answer\"])\\n    return str(response[\"answer\"])\\n\\n\\n\\n\\nif __name__ == \\'__main__\\':\\n    app.run(host=\"0.0.0.0\", port= 8080, debug= True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name = 'Generative AI Project',\\n    version= '0.0.0',\\n    author= 'Amir Khan',\\n    author_email= 'eamirkh360@gmail.com',\\n    packages= find_packages(),\\n    install_requires = []\\n\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\n#from langchain_pinecone import Pinecone\\n#from langchain.vectorstores import Pinecone\\n\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\n#PINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\n#os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='PINECONE_API_KEY=os.environ.get(\\'PINECONE_API_KEY\\')\\nOPENAI_API_KEY=os.environ.get(\\'OPENAI_API_KEY\\')\\n\\nextracted_data=load_pdf_file(data=\\'Data/\\')\\ntext_chunks=text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\nindex_name = \"medicalbot\"\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\n#pc = Pinecone(pinecone_api_key=PINECONE_API_KEY,embedding=embeddings,index_name=index_name)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='pc.create_index(\\n    name=index_name,\\n    dimension=384, \\n    metric=\"cosine\", \\n    spec=ServerlessSpec(\\n        cloud=\"aws\", \\n        region=\"us-east-1\"\\n    ) \\n) \\n\\nfrom langchain.vectorstores import Pinecone\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = Pinecone.from_documents(\\n    documents=text_chunks,\\n    index_name=index_name,\\n    embedding=embeddings, \\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\",\\n   \" test.py\"\\n]\\n\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n\\n    else:\\n        logging.info(f\"{filename} is already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='#from langchain.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\n#from langchain.embeddings import HuggingFaceEmbeddings\\n\\nfrom langchain_community.document_loaders import PyPDFLoader,DirectoryLoader\\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\\n\\n#Extract Data From the PDF File'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='def load_pdf_file(data):\\n    loader= DirectoryLoader(data,\\n                            glob=\"*.pdf\",\\n                            loader_cls=PyPDFLoader)\\n\\n    documents=loader.load()\\n\\n    return documents\\n\\n\\n\\n#Split the Data into Text Chunks\\ndef text_split(extracted_data):\\n    text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks=text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n\\n\\n#Download the Embeddings from HuggingFace'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')  #this model return 384 dimensions\\n    return embeddings\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\\n    \"You are an assistant for question-answering tasks. \"\\n    \"Use the following pieces of retrieved context to answer \"\\n    \"the question. If you don\\'t know the answer, say that you \"\\n    \"don\\'t know. Use three sentences maximum and keep the \"\\n    \"answer concise.\"\\n    \"\\\\n\\\\n\"\\n    \"{context}\"\\n)')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY=os.environ.get('OPENAI_API_KEY')\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings(disallowed_special=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is download_hugging_face_embeddings funtion?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirk\\AppData\\Local\\Temp\\ipykernel_9196\\79176006.py:1: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa(question)\n",
      "Number of requested results 20 is greater than number of elements in index 15, updating n_results = 15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `download_hugging_face_embeddings` function is a function that downloads embeddings from the Hugging Face model called 'sentence-transformers/all-MiniLM-L6-v2', which returns embeddings with 384 dimensions.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `text_split` function is responsible for splitting the extracted text data into smaller text chunks. It uses a Recursive Character Text Splitter to divide the text into manageable pieces for further processing or analysis.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is text_split?\"\n",
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
